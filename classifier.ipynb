{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() \n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from skimage import transform\n",
    "import tensorflow_addons as tfa\n",
    "import scipy.ndimage as ndimage\n",
    "keras = tf.keras\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileList(data_dir):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    class_names = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"], dtype='<U10')\n",
    "    num_files = len(list(data_dir.glob('*/*.jpg'))) + len(list(data_dir.glob('*/*.png')))\n",
    "    return tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=True), num_files, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_ds, image_count, CLASS_NAMES = getFileList(\"./training/\")\n",
    "list_ds_test, test_image_count, TEST_CLASS_NAMES = getFileList(\"./testing/\") ##getFileList(\"./ads_testing/\")\n",
    "list_ds_eval, eval_image_count, EVAL_CLASS_NAMES = getFileList(\"./eval/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH    = int(np.ceil(image_count/BATCH_SIZE))\n",
    "STEPS_PER_TEST     = int(np.ceil(test_image_count/BATCH_SIZE))\n",
    "STEPS_PER_EVAL     = int(np.ceil(eval_image_count/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)#, result_type='RaggedTensor')\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate_image(image):\n",
    "  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_random_rotate_image(image, label):\n",
    "  im_shape = image.shape\n",
    "  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
    "  image.set_shape(im_shape)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  #rotated_images = tfa.image.rotate(img, random_angles)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "  print(file_path)\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "labeled_ds = labeled_ds.map(tf_random_rotate_image)\n",
    "labeled_ds_test     = list_ds_test.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "labeled_ds_eval     = list_ds_eval.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=False, shuffle_buffer_size=1000, shuffle=True):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    if shuffle == True:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=False)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find invalid images\n",
    "def checkFiles(folder):\n",
    "    filenames = glob.glob(folder+\"/*jpg\") #glob.glob(\"Results/*.png\")\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        img = tf.io.read_file(filename)\n",
    "        tf.image.decode_jpeg(img, channels=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds    = prepare_for_training(labeled_ds)\n",
    "test_ds     = prepare_for_training(labeled_ds_test, shuffle=False)\n",
    "eval_ds     = prepare_for_training(labeled_ds_eval, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = keras.layers.Dense(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=eval_ds,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_steps=STEPS_PER_EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "path = str(time.time()).replace('.', '') + \"/\"\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "model.save(path + 'model.h5')\n",
    "#model.load('models/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savecm(ds, filename, iterations):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for i in range(iterations):\n",
    "        image_batch, label_batch = next(iter(ds))\n",
    "        predictions = predictions + list(model.predict_classes([image_batch, label_batch]))\n",
    "        labels = labels + list(np.argmax(label_batch, axis=1))\n",
    "    print(tf.math.confusion_matrix(labels, predictions))\n",
    "    c = np.array(tf.math.confusion_matrix(labels, predictions)).astype(np.float)\n",
    "    p = np.array(tf.math.confusion_matrix(labels, predictions)).astype(np.float)\n",
    "    #c = np.array(c) / np.array(c).astype(np.float).sum(axis=1)\n",
    "    sums = c.sum(axis=1)\n",
    "    sums_p = p.sum(axis=0)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            c[i, j] = c[i, j] / float(sums[i])\n",
    "            p[i, j] = p[i, j] / float(sums_p[j])\n",
    "\n",
    "    print(\"Recall\")            \n",
    "    sns.heatmap(c, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, annot=True)\n",
    "    plt.yticks(rotation=0) \n",
    "    plt.savefig(path + filename+\"recall.png\")\n",
    "    plt.show()  \n",
    "    pd.DataFrame(c).to_csv(path+filename+\"recall.csv\")\n",
    "    print(\"Precission\")\n",
    "    sns.heatmap(p, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, annot=True)\n",
    "    plt.yticks(rotation=0) \n",
    "    plt.savefig(path + filename+\"precission.png\")\n",
    "    plt.show()  \n",
    "    pd.DataFrame(p).to_csv(path+filename+\"precission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savecm(test_ds, 'cm_test', STEPS_PER_TEST)\n",
    "savecm(eval_ds, 'cm_eval', STEPS_PER_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).to_csv(path+'history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.legend(['train', 'validation'], loc='lower right', fontsize=16)\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.legend(['train', 'validation'], loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
